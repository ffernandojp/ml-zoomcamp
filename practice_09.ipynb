{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-09 21:31:58--  https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx.data\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving github.com (github.com)... 4.228.31.150\n",
      "Connecting to github.com (github.com)|4.228.31.150|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://release-assets.githubusercontent.com/github-production-release-asset/426348925/398ded4a-c41c-4e5a-9672-acb7e441de54?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-10T01%3A24%3A44Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx.data&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-10T00%3A24%3A43Z&ske=2025-12-10T01%3A24%3A44Z&sks=b&skv=2018-11-09&sig=CbZ2s9TBv3q%2Bf4x%2BvSChR1Q97GW51j2trKDPnAjJQbg%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTMyODUwMCwibmJmIjoxNzY1MzI2NzAwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.LMp838MwuDn_cqL2VW587p8QeQ8hi21APX-a4yKmxAQ&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx.data&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-12-09 21:32:03--  https://release-assets.githubusercontent.com/github-production-release-asset/426348925/398ded4a-c41c-4e5a-9672-acb7e441de54?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-10T01%3A24%3A44Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx.data&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-10T00%3A24%3A43Z&ske=2025-12-10T01%3A24%3A44Z&sks=b&skv=2018-11-09&sig=CbZ2s9TBv3q%2Bf4x%2BvSChR1Q97GW51j2trKDPnAjJQbg%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTMyODUwMCwibmJmIjoxNzY1MzI2NzAwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.LMp838MwuDn_cqL2VW587p8QeQ8hi21APX-a4yKmxAQ&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx.data&response-content-type=application%2Foctet-stream\n",
      "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 80355328 (77M) [application/octet-stream]\n",
      "Saving to: ‘hair_classifier_v1.onnx.data.3’\n",
      "\n",
      "hair_classifier_v1. 100%[===================>]  76.63M  8.72MB/s    in 9.9s    \n",
      "\n",
      "2025-12-09 21:32:13 (7.73 MB/s) - ‘hair_classifier_v1.onnx.data.3’ saved [80355328/80355328]\n",
      "\n",
      "--2025-12-09 21:32:13--  https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Resolving github.com (github.com)... 4.228.31.150\n",
      "Connecting to github.com (github.com)|4.228.31.150|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://release-assets.githubusercontent.com/github-production-release-asset/426348925/c6b83ad5-a901-40e9-bf2c-41ad174c870c?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-10T01%3A26%3A32Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-10T00%3A26%3A14Z&ske=2025-12-10T01%3A26%3A32Z&sks=b&skv=2018-11-09&sig=643M6fC1HXJoASvsva47sleYffVvnoxQL3Trjo9Xo4o%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTMyNzAxMywibmJmIjoxNzY1MzI2NzEzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.6bmPdbsclU9-iGX4bqwav1vL70YAHgkqXN1TqgB1Co0&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-12-09 21:32:14--  https://release-assets.githubusercontent.com/github-production-release-asset/426348925/c6b83ad5-a901-40e9-bf2c-41ad174c870c?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-10T01%3A26%3A32Z&rscd=attachment%3B+filename%3Dhair_classifier_v1.onnx&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-10T00%3A26%3A14Z&ske=2025-12-10T01%3A26%3A32Z&sks=b&skv=2018-11-09&sig=643M6fC1HXJoASvsva47sleYffVvnoxQL3Trjo9Xo4o%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTMyNzAxMywibmJmIjoxNzY1MzI2NzEzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.6bmPdbsclU9-iGX4bqwav1vL70YAHgkqXN1TqgB1Co0&response-content-disposition=attachment%3B%20filename%3Dhair_classifier_v1.onnx&response-content-type=application%2Foctet-stream\n",
      "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10337 (10K) [application/octet-stream]\n",
      "Saving to: ‘hair_classifier_v1.onnx.3’\n",
      "\n",
      "hair_classifier_v1. 100%[===================>]  10.09K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2025-12-09 21:32:14 (3.67 MB/s) - ‘hair_classifier_v1.onnx.3’ saved [10337/10337]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "# Define model and data URL\n",
    "PREFIX = \"https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle\"\n",
    "DATA_URL = f\"{PREFIX}/hair_classifier_v1.onnx.data\"\n",
    "MODEL_URL = f\"{PREFIX}/hair_classifier_v1.onnx\"\n",
    "\n",
    "# Download the files\n",
    "!wget {DATA_URL}\n",
    "!wget {MODEL_URL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e6ff43a",
    "outputId": "624d96ed-0713-4622-a455-5d0891de7f3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output node name is: output\n"
     ]
    }
   ],
   "source": [
    "# Load the ONNX model and determine the output node name\n",
    "sess = onnxruntime.InferenceSession(\"hair_classifier_v1.onnx\")\n",
    "output_name = sess.get_outputs()[0].name\n",
    "print(f\"The output node name is: {output_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib import request\n",
    "import numpy as np\n",
    "\n",
    "# Helper functions provided in the homework\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "    return img\n",
    "\n",
    "def prepare_image(img, target_size):\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    # The homework uses Image.NEAREST for resizing\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2d47cfc",
    "outputId": "5fd1c317-204d-4bbb-e209-5d3ea5e65402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input Name: input\n",
      "Model Input Shape: ['s77', 3, 200, 200]\n",
      "Model Output Name: output\n"
     ]
    }
   ],
   "source": [
    "# Load the ONNX model\n",
    "sess = onnxruntime.InferenceSession(\"hair_classifier_v1.onnx\")\n",
    "\n",
    "# Get input and output details from the model\n",
    "input_details = sess.get_inputs()[0]\n",
    "output_details = sess.get_outputs()[0]\n",
    "\n",
    "input_name = input_details.name\n",
    "input_shape = input_details.shape\n",
    "output_name = output_details.name\n",
    "\n",
    "print(f\"Model Input Name: {input_name}\")\n",
    "print(f\"Model Input Shape: {input_shape}\")\n",
    "print(f\"Model Output Name: {output_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 2 - Determined target image size: 200x200\n"
     ]
    }
   ],
   "source": [
    "# Question 2: Determine target image size from model input shape\n",
    "# Assuming a 4-dimensional input shape (batch, height, width, channels) or (batch, channels, height, width)\n",
    "if len(input_shape) == 4:\n",
    "    # Determine if channels are first or last to extract Height and Width\n",
    "    # Common PyTorch/ONNX models use (N, C, H, W), so C=input_shape[1]\n",
    "    # Common TensorFlow/Keras models use (N, H, W, C), so C=input_shape[3]\n",
    "    if input_shape[1] == 3: # Channels first (N, C, H, W)\n",
    "        target_height = input_shape[2]\n",
    "        target_width = input_shape[3]\n",
    "        channels_first = True\n",
    "    elif input_shape[3] == 3: # Channels last (N, H, W, C)\n",
    "        target_height = input_shape[1]\n",
    "        target_width = input_shape[2]\n",
    "        channels_first = False\n",
    "    else: # Fallback for unexpected channel counts/order, assume H, W are the next two dims after batch\n",
    "        target_height = input_shape[1]\n",
    "        target_width = input_shape[2]\n",
    "        channels_first = False # Default assumption if channels are not 3\n",
    "else:\n",
    "    # Fallback if input shape is not 4D, or if unable to infer. \n",
    "    # Based on common practice for similar models, 150x150 is a frequent size.\n",
    "    target_height = 150\n",
    "    target_width = 150\n",
    "    channels_first = False # Default assumption\n",
    "\n",
    "target_size = (target_width, target_height) # PIL resize expects (width, height)\n",
    "print(f\"\\nQuestion 2 - Determined target image size: {target_width}x{target_height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading image from: https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\n",
      "Image downloaded and prepared to size: (200, 200)\n"
     ]
    }
   ],
   "source": [
    "# Download and prepare the image\n",
    "img_url = \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\"\n",
    "print(f\"Downloading image from: {img_url}\")\n",
    "original_img = download_image(img_url)\n",
    "prepared_img = prepare_image(original_img, target_size)\n",
    "print(f\"Image downloaded and prepared to size: {prepared_img.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image converted to NumPy array and pre-processed for model input.\n",
      "Question 3 - Value of the first pixel (R channel) after pre-processing: -0.5215686559677124\n"
     ]
    }
   ],
   "source": [
    "# Question 3: Convert to NumPy array, pre-process, and get first pixel R channel\n",
    "# Convert PIL image to numpy array\n",
    "x = np.array(prepared_img, dtype=np.float32)\n",
    "\n",
    "# Adjust shape if channels first is required by the model\n",
    "if channels_first:\n",
    "    x = x.transpose(2, 0, 1) # From (H, W, C) to (C, H, W)\n",
    "\n",
    "# Add batch dimension (model expects batch size 1)\n",
    "x = np.expand_dims(x, axis=0) # From (C, H, W) to (1, C, H, W) or (H, W, C) to (1, H, W, C)\n",
    "\n",
    "# Pre-processing: normalize pixel values to [-1, 1]\n",
    "# (e.g., x / 127.5 - 1.0 for 0-255 pixel range)\n",
    "x = x / 127.5 - 1.0\n",
    "print(\"Image converted to NumPy array and pre-processed for model input.\")\n",
    "\n",
    "# Get the value of the first pixel (top-left, R channel) after pre-processing\n",
    "# The R channel is the first channel (index 0) regardless of channels_first/last\n",
    "first_pixel_r_value = x[0, 0, 0, 0]\n",
    "print(f\"Question 3 - Value of the first pixel (R channel) after pre-processing: {first_pixel_r_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 4 - Model output: -0.203227236866951\n"
     ]
    }
   ],
   "source": [
    "# Question 4: Perform model inference\n",
    "input_feed = {input_name: x}\n",
    "outputs = sess.run([output_name], input_feed)\n",
    "\n",
    "# Return the model outputs a single scalar prediction\n",
    "model_prediction = outputs[0][0][0]\n",
    "print(f\"Question 4 - Model output: {model_prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba023ac8"
   },
   "source": [
    "Questions 5 and 6: Create the lambda function file and Dockerfile, then we determine the Docker base image size and perform Docker-based inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95a381d6",
    "outputId": "0f278d1e-1d85-4561-80bd-7ab203a93830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lambda_function.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lambda_function.py\n",
    "import onnxruntime\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib import request\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "def download_image(url):\n",
    "    \"\"\"\n",
    "    Download an image from a given URL and open it as a PIL Image.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the image to download.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: The downloaded image.\n",
    "    \"\"\"\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "    return img\n",
    "\n",
    "def prepare_image(img, target_size):\n",
    "    \"\"\"\n",
    "    Prepare the image for model input by converting to RGB and resizing.\n",
    "\n",
    "    Args:\n",
    "        img (PIL.Image): Input image.\n",
    "        target_size (tuple): Desired size as (width, height).\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: Processed image.\n",
    "    \"\"\"\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img\n",
    "\n",
    "class ImageClassifier:\n",
    "    \"\"\"\n",
    "    Image classifier using an ONNX model for inference.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path=\"hair_classifier_empty.onnx\"):\n",
    "        \"\"\"\n",
    "        Initialize the ONNX Runtime session and read model input/output metadata.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the ONNX model file.\n",
    "        \"\"\"\n",
    "        self.sess = onnxruntime.InferenceSession(model_path)\n",
    "        self.input_name = self.sess.get_inputs()[0].name\n",
    "        self.input_shape = self.sess.get_inputs()[0].shape\n",
    "        self.output_name = self.sess.get_outputs()[0].name\n",
    "\n",
    "        # Determine target input size and channel format (channels first or last)\n",
    "        if len(self.input_shape) == 4:\n",
    "            if self.input_shape[1] == 3:  # Channels first (N, C, H, W)\n",
    "                self.target_height = self.input_shape[2]\n",
    "                self.target_width = self.input_shape[3]\n",
    "                self.channels_first = True\n",
    "            elif self.input_shape[3] == 3:  # Channels last (N, H, W, C)\n",
    "                self.target_height = self.input_shape[1]\n",
    "                self.target_width = self.input_shape[2]\n",
    "                self.channels_first = False\n",
    "            else:\n",
    "                raise ValueError(\"Model input shape does not indicate 3 channels correctly.\")\n",
    "        else:\n",
    "            raise ValueError(\"Model input shape is not 4-dimensional.\")\n",
    "\n",
    "        self.target_size = (self.target_width, self.target_height)\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        \"\"\"\n",
    "        Convert the input PIL image to a normalized numpy tensor suitable for the model.\n",
    "\n",
    "        Args:\n",
    "            img (PIL.Image): Preprocessed image of target size.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Normalized input tensor with correct shape.\n",
    "        \"\"\"\n",
    "        x = np.array(img, dtype=np.float32)\n",
    "        # Change shape to channels first if model expects it\n",
    "        if self.channels_first:\n",
    "            x = x.transpose(2, 0, 1)\n",
    "        x = np.expand_dims(x, axis=0)  # Add batch dimension\n",
    "        x = x / 127.5 - 1.0  # Normalize pixel values to [-1, 1]\n",
    "        return x\n",
    "\n",
    "    def predict(self, url):\n",
    "        \"\"\"\n",
    "        Download an image from the URL, preprocess it, and run model inference.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL of the image to classify.\n",
    "\n",
    "        Returns:\n",
    "            float: Prediction result from the model.\n",
    "        \"\"\"\n",
    "        original_img = download_image(url)\n",
    "        prepared_img = prepare_image(original_img, self.target_size)\n",
    "        processed_x = self.preprocess(prepared_img)\n",
    "        input_feed = {self.input_name: processed_x}\n",
    "        outputs = self.sess.run([self.output_name], input_feed)\n",
    "        return outputs[0][0][0]\n",
    "\n",
    "# Instantiate classifier with the ONNX model file\n",
    "classifier = ImageClassifier(model_path=\"hair_classifier_empty.onnx\")\n",
    "\n",
    "def handler(event, context=None):\n",
    "    \"\"\"\n",
    "    AWS Lambda handler function to process incoming event, extract image URL,\n",
    "    perform prediction, and return response.\n",
    "\n",
    "    Args:\n",
    "        event (dict): Event payload containing image URL data.\n",
    "        context: Lambda context (not used).\n",
    "\n",
    "    Returns:\n",
    "        dict: HTTP-like response with prediction or error details.\n",
    "    \"\"\"\n",
    "    url = None\n",
    "\n",
    "    # Parse URL from event: from JSON body or directly from event keys\n",
    "    if isinstance(event, dict):\n",
    "        if 'body' in event and isinstance(event['body'], str):\n",
    "            try:\n",
    "                body = json.loads(event['body'])\n",
    "                url = body.get('url')\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        elif 'url' in event:\n",
    "            url = event.get('url')\n",
    "\n",
    "    if not url:\n",
    "        return {\n",
    "            \"statusCode\": 400,\n",
    "            \"body\": json.dumps({\"error\": \"Invalid event format or image URL not provided\"})\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        prediction = classifier.predict(url)\n",
    "        return {\n",
    "            \"statusCode\": 200,\n",
    "            \"body\": json.dumps({\"prediction\": float(prediction)})\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return {\n",
    "            \"statusCode\": 500,\n",
    "            \"body\": json.dumps({\"error\": str(e)})\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c486536",
    "outputId": "83913aad-5f14-4a46-89f8-0a2792917329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM agrigorev/model-2025-hairstyle:v1\n",
    "\n",
    "RUN pip install --no-cache-dir onnxruntime numpy Pillow==11.3.0\n",
    "\n",
    "COPY lambda_function.py ${LAMBDA_TASK_ROOT}/\n",
    "\n",
    "CMD [ \"lambda_function.handler\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1: Pulling from agrigorev/model-2025-hairstyle\n",
      "Digest: sha256:9e43d5a5323f7f07688c0765d3c0137af66d0154af37833ed721d6b4de6df528\n",
      "Status: Image is up to date for agrigorev/model-2025-hairstyle:v1\n",
      "docker.io/agrigorev/model-2025-hairstyle:v1\n",
      "                                                            \u001b[1m\u001b[106m\u001b[30mi\u001b[0m\u001b[0m \u001b[96mInfo → \u001b[0m\u001b[0m \u001b[38;5;0m\u001b[48;5;14m U \u001b[0m In Use\n",
      "\u001b[39m\u001b[1mIMAGE\u001b[0m                           \u001b[39m\u001b[1mID\u001b[0m             \u001b[39m\u001b[1mDISK USAGE\u001b[0m   \u001b[39m\u001b[1mCONTENT SIZE\u001b[0m   \u001b[39m\u001b[1mEXTRA\u001b[0m\n",
      "\u001b[34m\u001b[1magrigorev/model-2025-hairstyle:v1\u001b[0m\n",
      "                                \u001b[39m4528ad1525d5\u001b[0m        \u001b[39m608MB\u001b[0m             \u001b[39m0B\u001b[0m   \u001b[38;5;0m\u001b[48;5;14m U \u001b[0m  \n"
     ]
    }
   ],
   "source": [
    "!docker pull agrigorev/model-2025-hairstyle:v1\n",
    "!docker images agrigorev/model-2025-hairstyle:v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5: Determine the Docker image size\n",
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPRECATED: The legacy builder is deprecated and will be removed in a future release.\n",
      "            Install the buildx component to build images with BuildKit:\n",
      "            https://docs.docker.com/go/buildx/\n",
      "\n",
      "Sending build context to Docker daemon  762.6MB\n",
      "Step 1/4 : FROM agrigorev/model-2025-hairstyle:v1\n",
      " ---> 4528ad1525d5\n",
      "Step 2/4 : RUN pip install --no-cache-dir onnxruntime numpy Pillow==11.3.0\n",
      " ---> Using cache\n",
      " ---> 415cfe7fe653\n",
      "Step 3/4 : COPY lambda_function.py ${LAMBDA_TASK_ROOT}/\n",
      " ---> Using cache\n",
      " ---> c1f7af96301f\n",
      "Step 4/4 : CMD [ \"lambda_function.handler\" ]\n",
      " ---> Running in 29f3f60a3a80\n",
      " ---> Removed intermediate container 29f3f60a3a80\n",
      " ---> 84da9517d0eb\n",
      "Successfully built 84da9517d0eb\n",
      "Successfully tagged hair-classifier-lambda:latest\n",
      "10 Dec 2025 00:14:17,741 [INFO] (rapid) exec '/var/runtime/bootstrap' (cwd=/var/task, handler=)\n",
      "START RequestId: af35da4b-5e93-4c39-885b-6ed7d87d4e97 Version: $LATEST\n",
      "10 Dec 2025 00:16:34,988 [INFO] (rapid) INIT START(type: on-demand, phase: init)\n",
      "10 Dec 2025 00:16:34,988 [INFO] (rapid) The extension's directory \"/opt/extensions\" does not exist, assuming no extensions to be loaded.\n",
      "10 Dec 2025 00:16:34,988 [INFO] (rapid) Starting runtime without AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN , Expected?: false\n",
      "10 Dec 2025 00:16:35,160 [INFO] (rapid) INIT RTDONE(status: success)\n",
      "10 Dec 2025 00:16:35,160 [INFO] (rapid) INIT REPORT(durationMs: 171.812000)\n",
      "10 Dec 2025 00:16:35,160 [INFO] (rapid) INVOKE START(requestId: 2ebd62e2-a7ad-408e-a96f-a539ea5b88e7)\n",
      "10 Dec 2025 00:16:37,531 [INFO] (rapid) INVOKE RTDONE(status: success, produced bytes: 0, duration: 2370.883000ms)\n",
      "END RequestId: 2ebd62e2-a7ad-408e-a96f-a539ea5b88e7\n",
      "REPORT RequestId: 2ebd62e2-a7ad-408e-a96f-a539ea5b88e7\tInit Duration: 0.03 ms\tDuration: 2542.83 ms\tBilled Duration: 2543 ms\tMemory Size: 3008 MB\tMax Memory Used: 3008 MB\t\n",
      "^C\n",
      "10 Dec 2025 00:18:04,768 [INFO] (rapid) Received signal signal=interrupt\n",
      "10 Dec 2025 00:18:04,768 [INFO] (rapid) Shutting down...\n",
      "10 Dec 2025 00:18:04,769 [WARNING] (rapid) Reset initiated: SandboxTerminated\n",
      "10 Dec 2025 00:18:04,769 [INFO] (rapid) Sending SIGKILL to runtime-1(18).\n",
      "10 Dec 2025 00:18:04,773 [INFO] (rapid) Waiting for runtime domain processes termination\n"
     ]
    }
   ],
   "source": [
    "# Question 6: Build and run Dockerfile\n",
    "!docker build -t hair-classifier-lambda .\n",
    "!docker run -p 8080:8080 hair-classifier-lambda:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"statusCode\": 200, \"body\": \"{\\\"prediction\\\": -0.09092357009649277}\"}"
     ]
    }
   ],
   "source": [
    "# Model output (docker)\n",
    "!curl -XPOST \"http://localhost:8080/2015-03-31/functions/function/invocations\" -d '{\"url\": \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\"}'"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
